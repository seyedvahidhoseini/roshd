{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nv_dHp-7Y1v6",
        "outputId": "26f3c1b3-44b6-4e27-8e31-8f8aff4d519c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\software2\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:3550: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.messages import AIMessage\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from hazm import Normalizer, word_tokenize\n",
        "import faiss\n",
        "import torch\n",
        "import pickle\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2kxmoMWZAJL",
        "outputId": "ebb7c03c-1c6f-47c0-b9d3-f07eba794c0f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\software2\\AppData\\Local\\Temp\\2\\ipykernel_3168\\1197029383.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(model='nvidia/llama-3.1-nemotron-70b-instruct' , temperature=0.3, api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "model_id = \"vahidhoseini/qwen-roshdv1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA9WZmH3sGvb",
        "outputId": "de3d3c99-b3a9-4065-fd9e-8f808dc7dd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The PDF contains 101 pages.\n"
          ]
        }
      ],
      "source": [
        "loader = PyPDFLoader(\"C:/Users/software2/Desktop/chatbot/file.pdf\")\n",
        "docs = loader.load()\n",
        "num_pages = len(docs)\n",
        "print(f\"The PDF contains {num_pages} pages.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1ouJJeI_mY0"
      },
      "source": [
        "#more perftional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWPlNtT8_Ffc"
      },
      "source": [
        "#more perfetional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfvvd62U_IUU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\software2\\AppData\\Local\\Temp\\2\\ipykernel_3168\\3550581144.py:30: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=model_name, encode_kwargs={\n",
            "c:\\Users\\software2\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if isinstance(docs[0], str):\n",
        "    docs = [Document(page_content=text) for text in docs]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \".\", \"؛\", \";\", \" \", \",\", \"-\"],\n",
        "    keep_separator=True,\n",
        "    length_function=lambda x: len(x.split())\n",
        ")\n",
        "\n",
        "normalizer = Normalizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"پیش‌پردازش متن: نرمال‌سازی و توکن‌سازی.\"\"\"\n",
        "    try:\n",
        "        text = text.lower()\n",
        "        text = normalizer.normalize(text)\n",
        "        text = ''.join(e for e in text if e.isalnum() or e.isspace())\n",
        "        tokens = word_tokenize(text)\n",
        "        return ' '.join(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocessing: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "splits = text_splitter.split_documents(docs)\n",
        "texts = [preprocess(doc.page_content) for doc in splits]\n",
        "\n",
        "model_name = \"Msobhi/Persian_Sentence_Embedding_v3\"\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=model_name, encode_kwargs={\n",
        "    \"batch_size\": 16,\n",
        "    \"normalize_embeddings\": True,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"padding\": True,\n",
        "    \"truncation\": True,\n",
        "    \"max_length\": 512\n",
        "})\n",
        "\n",
        "embeddings_result = embedding_model.embed_documents(texts)\n",
        "\n",
        "\n",
        "with open('embeddings_result.pkl', 'wb') as file:\n",
        "    pickle.dump(embeddings_result, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7UdjPsKyr1r",
        "outputId": "5176e42f-3649-4b22-eb58-419e0133805e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "FAISS_SETTINGS = {\n",
        "    \"metric\": \"L2\",  \n",
        "    \"index_type\": \"Flat\",  \n",
        "    \"k\": 10  \n",
        "}\n",
        "\n",
        "d = len(embeddings_result[0])\n",
        "\n",
        "if FAISS_SETTINGS[\"metric\"] == \"L2\":\n",
        "    metric = faiss.METRIC_L2\n",
        "elif FAISS_SETTINGS[\"metric\"] == \"IP\":\n",
        "    metric = faiss.METRIC_INNER_PRODUCT\n",
        "elif FAISS_SETTINGS[\"metric\"] == \"Cosine\":\n",
        "    metric = faiss.METRIC_INNER_PRODUCT\n",
        "    embeddings_result = [vec / (sum(v ** 2 for v in vec) ** 0.5) for vec in embeddings_result]\n",
        "\n",
        "if FAISS_SETTINGS[\"index_type\"] == \"Flat\":\n",
        "    index = faiss.IndexFlat(d, metric)\n",
        "else:\n",
        "    raise ValueError(f\"Only 'Flat' index type is supported in this version.\")\n",
        "\n",
        "docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(splits)})\n",
        "\n",
        "index_to_docstore_id = {i: str(i) for i in range(len(splits))}\n",
        "\n",
        "vectorstore = FAISS(\n",
        "    embedding_model.embed_query,\n",
        "    index,\n",
        "    docstore,\n",
        "    index_to_docstore_id\n",
        ")\n",
        "\n",
        "vectorstore.add_texts(texts, metadatas=[{\"source\": f\"doc_{i}\"} for i in range(len(texts))])\n",
        "\n",
        "faiss.write_index(vectorstore.index, \"faiss_index.index\")\n",
        "\n",
        "with open(\"faiss_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump({\n",
        "        \"docstore\": vectorstore.docstore,\n",
        "        \"index_to_docstore_id\": vectorstore.index_to_docstore_id\n",
        "    }, f)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### embding_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('embeddings_result.pkl', 'rb') as file:\n",
        "    embeddings_resultt = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### vectordatabase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "source": [
        "faiss_index = faiss.read_index(\"faiss_index.index\")\n",
        "\n",
        "with open(\"faiss_data.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "vectorstore = FAISS(\n",
        "    embedding_function=embedding_model.embed_query,  \n",
        "    index=faiss_index,\n",
        "    docstore=data[\"docstore\"],\n",
        "    index_to_docstore_id=data[\"index_to_docstore_id\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",  \n",
        "    search_kwargs={\"k\": 10}    \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028BA65ABD90>, search_kwargs={'k': 10})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "c7Yqw35Dszp6"
      },
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"تو یک دستیار خوب برای سوال و جواب هستی. \"\n",
        "    \"از context و سوال و جواب‌های موجود برای پاسخ دادن استفاده کن. \"\n",
        "    \"لطفاً تنها اطلاعات مرتبط با سوال را از context استخراج کن و از اطلاعات بی‌ربط خودداری کن. \"\n",
        "    \"اگر جواب سوالی را نمی‌دانی، به جای اینکه بگویی 'نمی دانم'، بگو که 'پاسخ دقیقی برای این سوال ندارم' یا درخواست اطلاعات بیشتر کن.\"\n",
        "    \"به زبان فارسی جواب بده.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BYwJcLRKs11g"
      },
      "outputs": [],
      "source": [
        "question_answer_chain=create_stuff_documents_chain(llm,prompt)\n",
        "rag_chain=create_retrieval_chain(retriever,question_answer_chain)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKtYs-5WwN2Q"
      },
      "source": [
        "# output "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "query = ' برنامه نویسی پایتون بلدی؟'\n",
        "result = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "answer = result[\"answer\"]\n",
        "print(\"Answer:\", answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
